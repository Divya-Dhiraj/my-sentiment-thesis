{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e243534e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katik\\AppData\\Local\\Temp\\ipykernel_26056\\1553122191.py:13: DtypeWarning: Columns (18,20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the data. Here are the first 5 rows with all columns visible:\n",
      "   gl_product_group           gl_name subcategory_code  \\\n",
      "0               265  Major Appliances         26504030   \n",
      "1               265  Major Appliances         26501100   \n",
      "2               265  Major Appliances         26503115   \n",
      "3               265  Major Appliances         26502040   \n",
      "4               265  Major Appliances         26504030   \n",
      "\n",
      "                             subcategory_description     new_pgrollup  \\\n",
      "0       4030 Dishwasher Parts  Accessories & Filters  Other Hardlines   \n",
      "1  1100 Fridge & Freezer Parts  Accessories and F...  Other Hardlines   \n",
      "2  3115 Cooker  Oven and Microwave Parts  Accesso...  Other Hardlines   \n",
      "3        2040 Washing and Drying Machine accessories  Other Hardlines   \n",
      "4       4030 Dishwasher Parts  Accessories & Filters  Other Hardlines   \n",
      "\n",
      "             category manufacturer_name          product_type  marketplace_id  \\\n",
      "0    4000 Dishwashers           Unknown            DISHWASHER               4   \n",
      "1  1000 Refrigeration           Unknown  POTABLE_WATER_FILTER               4   \n",
      "2        3000 Cooking           Unknown            LIGHT_BULB               4   \n",
      "3        2000 Laundry           Unknown     LAUNDRY_APPLIANCE               4   \n",
      "4    4000 Dishwashers           Unknown        CLEANING_AGENT               4   \n",
      "\n",
      "         asin fulfillment_channel  year  month  defect_category root_cause  \\\n",
      "0  B098K6C12Z                 FBA  2025      4  Fit/Style Issue  Fit/Style   \n",
      "1  B07WGS2NNC                 FBA  2024      1  Fit/Style Issue  Fit/Style   \n",
      "2  B01M7S6SRA                 FBA  2025      4  Fit/Style Issue  Fit/Style   \n",
      "3  B0BGQK683X                 FBA  2025      8  Fit/Style Issue  Fit/Style   \n",
      "4  B09J57TRP6                 FBA  2025      8  Fit/Style Issue  Fit/Style   \n",
      "\n",
      "  asp_band sort_type  customer_id concession_reason             ship_day  \\\n",
      "0  Low ASP      Sort   9896515435               NaN  2025-04-10 00:00:00   \n",
      "1  Low ASP      Sort     45300823               NaN  2024-01-13 00:00:00   \n",
      "2  Low ASP      Sort   6467398535               NaN  2025-04-08 00:00:00   \n",
      "3  Low ASP      Sort    589080392               NaN  2025-08-18 00:00:00   \n",
      "4  Low ASP      Sort  12901810512               NaN  2025-08-04 00:00:00   \n",
      "\n",
      "  concession_creation_day concession_bucket  our_price  \\\n",
      "0                     NaN               NaN      15.96   \n",
      "1                     NaN               NaN      13.44   \n",
      "2                     NaN               NaN       6.40   \n",
      "3                     NaN               NaN       7.97   \n",
      "4                     NaN               NaN      14.25   \n",
      "\n",
      "   our_price_discount_amt monetory_unit  shipped_units  shipped_cogs  \\\n",
      "0                     0.0           EUR              1           0.0   \n",
      "1                     0.0           EUR              1           0.0   \n",
      "2                     0.0           EUR              1           0.0   \n",
      "3                     0.0           EUR              1           0.0   \n",
      "4                     0.0           EUR              1           0.0   \n",
      "\n",
      "   product_gms  total_units_conceded  \n",
      "0        17.61                   0.0  \n",
      "1        14.83                   0.0  \n",
      "2         7.06                   0.0  \n",
      "3         8.79                   0.0  \n",
      "4        15.72                   0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Construct the correct file path\n",
    "file_path = os.path.join('..', 'data', 'processed_data.csv')\n",
    "\n",
    "# --- THIS IS THE FIX ---\n",
    "# Set the option to display ALL columns without truncation\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "try:\n",
    "    # Read the CSV using the correct tab separator\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    \n",
    "    # Now, this print statement will show all columns\n",
    "    print(\"Successfully loaded the data. Here are the first 5 rows with all columns visible:\")\n",
    "    print(df.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file was not found at the path: {os.path.abspath(file_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b3092a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katik\\AppData\\Local\\Temp\\ipykernel_12508\\171774188.py:14: DtypeWarning: Columns (18,20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File loaded successfully. Columns found:\n",
      "Index(['gl_product_group', 'gl_name', 'subcategory_code',\n",
      "       'subcategory_description', 'new_pgrollup', 'category',\n",
      "       'manufacturer_name', 'product_type', 'marketplace_id', 'asin',\n",
      "       'fulfillment_channel', 'year', 'month', 'defect_category', 'root_cause',\n",
      "       'asp_band', 'sort_type', 'customer_id', 'concession_reason', 'ship_day',\n",
      "       'concession_creation_day', 'concession_bucket', 'our_price',\n",
      "       'our_price_discount_amt', 'monetory_unit', 'shipped_units',\n",
      "       'shipped_cogs', 'product_gms', 'total_units_conceded'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ Here are the distinct concession (return) reasons and their total counts:\n",
      "concession_reason\n",
      "Incompatible or not useful for intended purpose    65487\n",
      "No reason given                                    48758\n",
      "Defective/Does not work properly                   37309\n",
      "Performance or quality not adequate                23858\n",
      "Accidental order                                   20195\n",
      "                                                   ...  \n",
      "CORCH_AMZ_PG_BAD_DESC                                  1\n",
      "BUYERAPOLO                                             1\n",
      "return_tracking_issue                                  1\n",
      "Arrived in addition to what was ordered                1\n",
      "LOSTSHIPMT                                             1\n",
      "Name: count, Length: 72, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- ACTION REQUIRED ---\n",
    "# Paste your full path between the quotes. The 'r' at the beginning is important!\n",
    "full_file_path = r\"C:\\projects\\my-local-thesis\\data\\processed_data.csv\"\n",
    "\n",
    "# -----------------------\n",
    "\n",
    "if \"YOUR_FULL_PATH_HERE\" in full_file_path:\n",
    "    print(\"❌ Please replace 'YOUR_FULL_PATH_HERE' with the actual full path to your file and run this cell again.\")\n",
    "else:\n",
    "    try:\n",
    "        # Read the CSV using the corrected path and tab separator\n",
    "        df = pd.read_csv(full_file_path, sep='\\t')\n",
    "\n",
    "        # --- DEBUGGING STEP ---\n",
    "        # Let's print the columns pandas found to make sure they were split correctly\n",
    "        print(\"✅ File loaded successfully. Columns found:\")\n",
    "        print(df.columns)\n",
    "        print(\"-\" * 50) # Adding a separator for clarity\n",
    "\n",
    "        # Now, we try to access the column\n",
    "        concession_reason_counts = df['concession_reason'].value_counts()\n",
    "\n",
    "        print(\"\\n✅ Here are the distinct concession (return) reasons and their total counts:\")\n",
    "        print(concession_reason_counts)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: The file was not found at the path you provided: {full_file_path}\")\n",
    "    except KeyError:\n",
    "        print(\"❌ Error: The column 'concession_reason' was not found.\")\n",
    "        print(\"Please check the column list printed above to find the correct column name.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6d4dd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katik\\AppData\\Local\\Temp\\ipykernel_12508\\461808814.py:5: DtypeWarning: Columns (18,20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_file_path, sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a new 'weekly_performance_df' from your real data:\n",
      "    ship_day        asin  total_units_sold  average_selling_price  \\\n",
      "0 2021-10-25  B0928ZH96F                 0                    NaN   \n",
      "1 2021-12-27  B09KV6F186                 0                    NaN   \n",
      "2 2022-01-10  B08NRBDMQW                 0                    NaN   \n",
      "3 2022-01-10  B08X893966                 0                    NaN   \n",
      "4 2022-01-17  B08KTJM954                 0                    NaN   \n",
      "\n",
      "   total_units_conceded  \n",
      "0                   1.0  \n",
      "1                   1.0  \n",
      "2                   1.0  \n",
      "3                   1.0  \n",
      "4                   1.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your real data using the full path\n",
    "full_file_path = r\"C:\\projects\\my-local-thesis\\data\\processed_data.csv\"\n",
    "df = pd.read_csv(full_file_path, sep='\\t')\n",
    "\n",
    "# --- DATA TRANSFORMATION ---\n",
    "# Your application needs a weekly summary. Let's create one.\n",
    "# First, ensure date columns are in the correct format.\n",
    "df['ship_day'] = pd.to_datetime(df['ship_day'])\n",
    "\n",
    "# Group the data by week and ASIN to create a performance summary\n",
    "# This mimics the 'weekly_product_performance.csv' your app expects\n",
    "weekly_performance_df = df.groupby([\n",
    "    pd.Grouper(key='ship_day', freq='W-MON'), # W-MON means group by week, starting on Monday\n",
    "    'asin'\n",
    "]).agg(\n",
    "    total_units_sold=('shipped_units', 'sum'),\n",
    "    average_selling_price=('our_price', 'mean'),\n",
    "    total_units_conceded=('total_units_conceded', 'sum') # Counting returns\n",
    ").reset_index()\n",
    "\n",
    "print(\"Created a new 'weekly_performance_df' from your real data:\")\n",
    "print(weekly_performance_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acce19ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully created a combined weekly performance summary:\n",
      "    ship_day        asin  total_units_sold  average_selling_price  \\\n",
      "0 2021-10-25  B0928ZH96F               0.0                    0.0   \n",
      "1 2021-12-27  B09KV6F186               0.0                    0.0   \n",
      "2 2022-01-10  B08NRBDMQW               0.0                    0.0   \n",
      "3 2022-01-10  B08X893966               0.0                    0.0   \n",
      "4 2022-01-17  B08KTJM954               0.0                    0.0   \n",
      "\n",
      "   total_units_conceded  \n",
      "0                   1.0  \n",
      "1                   1.0  \n",
      "2                   1.0  \n",
      "3                   1.0  \n",
      "4                   1.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your real data using the full path\n",
    "# low_memory=False is added to handle the DtypeWarning\n",
    "full_file_path = r\"C:\\projects\\my-local-thesis\\data\\processed_data.csv\"\n",
    "df = pd.read_csv(full_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "# Convert date column to datetime objects for proper grouping\n",
    "df['ship_day'] = pd.to_datetime(df['ship_day'])\n",
    "\n",
    "# 1. Split the DataFrame into orders and returns based on your business rule\n",
    "orders_df = df[df['concession_reason'].isnull()].copy()\n",
    "returns_df = df[df['concession_reason'].notnull()].copy()\n",
    "\n",
    "# 2. Aggregate the orders data to get sales figures\n",
    "weekly_sales = orders_df.groupby(\n",
    "    [pd.Grouper(key='ship_day', freq='W-MON'), 'asin']\n",
    ").agg(\n",
    "    total_units_sold=('shipped_units', 'sum'),\n",
    "    average_selling_price=('our_price', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# 3. Aggregate the returns data to get concession figures\n",
    "weekly_returns = returns_df.groupby(\n",
    "    [pd.Grouper(key='ship_day', freq='W-MON'), 'asin']\n",
    ").agg(\n",
    "    total_units_conceded=('total_units_conceded', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# 4. Merge the two summaries into one comprehensive weekly report\n",
    "weekly_performance_df = pd.merge(\n",
    "    weekly_sales,\n",
    "    weekly_returns,\n",
    "    on=['ship_day', 'asin'],\n",
    "    how='outer' # 'outer' merge keeps all records from both tables\n",
    ")\n",
    "\n",
    "# 5. Clean up the final DataFrame by filling empty values with 0\n",
    "weekly_performance_df.fillna(0, inplace=True)\n",
    "\n",
    "print(\"✅ Successfully created a combined weekly performance summary:\")\n",
    "print(weekly_performance_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ace95c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows classified as ORDERS: 3709223\n",
      "Number of rows classified as RETURNS: 253473\n",
      "--------------------------------------------------\n",
      "--- First 5 rows of the 'Orders' DataFrame ---\n",
      "   gl_product_group           gl_name subcategory_code  \\\n",
      "0               265  Major Appliances         26504030   \n",
      "1               265  Major Appliances         26501100   \n",
      "2               265  Major Appliances         26503115   \n",
      "3               265  Major Appliances         26502040   \n",
      "4               265  Major Appliances         26504030   \n",
      "\n",
      "                             subcategory_description     new_pgrollup  \\\n",
      "0       4030 Dishwasher Parts  Accessories & Filters  Other Hardlines   \n",
      "1  1100 Fridge & Freezer Parts  Accessories and F...  Other Hardlines   \n",
      "2  3115 Cooker  Oven and Microwave Parts  Accesso...  Other Hardlines   \n",
      "3        2040 Washing and Drying Machine accessories  Other Hardlines   \n",
      "4       4030 Dishwasher Parts  Accessories & Filters  Other Hardlines   \n",
      "\n",
      "             category manufacturer_name          product_type  marketplace_id  \\\n",
      "0    4000 Dishwashers           Unknown            DISHWASHER               4   \n",
      "1  1000 Refrigeration           Unknown  POTABLE_WATER_FILTER               4   \n",
      "2        3000 Cooking           Unknown            LIGHT_BULB               4   \n",
      "3        2000 Laundry           Unknown     LAUNDRY_APPLIANCE               4   \n",
      "4    4000 Dishwashers           Unknown        CLEANING_AGENT               4   \n",
      "\n",
      "         asin fulfillment_channel  year  month  defect_category root_cause  \\\n",
      "0  B098K6C12Z                 FBA  2025      4  Fit/Style Issue  Fit/Style   \n",
      "1  B07WGS2NNC                 FBA  2024      1  Fit/Style Issue  Fit/Style   \n",
      "2  B01M7S6SRA                 FBA  2025      4  Fit/Style Issue  Fit/Style   \n",
      "3  B0BGQK683X                 FBA  2025      8  Fit/Style Issue  Fit/Style   \n",
      "4  B09J57TRP6                 FBA  2025      8  Fit/Style Issue  Fit/Style   \n",
      "\n",
      "  asp_band sort_type  customer_id concession_reason             ship_day  \\\n",
      "0  Low ASP      Sort   9896515435               NaN  2025-04-10 00:00:00   \n",
      "1  Low ASP      Sort     45300823               NaN  2024-01-13 00:00:00   \n",
      "2  Low ASP      Sort   6467398535               NaN  2025-04-08 00:00:00   \n",
      "3  Low ASP      Sort    589080392               NaN  2025-08-18 00:00:00   \n",
      "4  Low ASP      Sort  12901810512               NaN  2025-08-04 00:00:00   \n",
      "\n",
      "  concession_creation_day concession_bucket  our_price  \\\n",
      "0                     NaN               NaN      15.96   \n",
      "1                     NaN               NaN      13.44   \n",
      "2                     NaN               NaN       6.40   \n",
      "3                     NaN               NaN       7.97   \n",
      "4                     NaN               NaN      14.25   \n",
      "\n",
      "   our_price_discount_amt monetory_unit  shipped_units  shipped_cogs  \\\n",
      "0                     0.0           EUR              1           0.0   \n",
      "1                     0.0           EUR              1           0.0   \n",
      "2                     0.0           EUR              1           0.0   \n",
      "3                     0.0           EUR              1           0.0   \n",
      "4                     0.0           EUR              1           0.0   \n",
      "\n",
      "   product_gms  total_units_conceded  \n",
      "0        17.61                   0.0  \n",
      "1        14.83                   0.0  \n",
      "2         7.06                   0.0  \n",
      "3         8.79                   0.0  \n",
      "4        15.72                   0.0  \n",
      "\n",
      "--- Let's look at the 'shipped_units' and 'our_price' in those orders ---\n",
      "   shipped_units  our_price\n",
      "0              1      15.96\n",
      "1              1      13.44\n",
      "2              1       6.40\n",
      "3              1       7.97\n",
      "4              1      14.25\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your real data\n",
    "full_file_path = r\"C:\\projects\\my-local-thesis\\data\\processed_data.csv\"\n",
    "df = pd.read_csv(full_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "# 1. Split the DataFrame using the same logic as before\n",
    "orders_df = df[df['concession_reason'].isnull()].copy()\n",
    "returns_df = df[df['concession_reason'].notnull()].copy()\n",
    "\n",
    "# 2. Print the number of rows in each new table\n",
    "print(f\"Number of rows classified as ORDERS: {orders_df.shape[0]}\")\n",
    "print(f\"Number of rows classified as RETURNS: {returns_df.shape[0]}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 3. Examine the 'Orders' DataFrame specifically\n",
    "if not orders_df.empty:\n",
    "    print(\"--- First 5 rows of the 'Orders' DataFrame ---\")\n",
    "    print(orders_df.head())\n",
    "    print(\"\\n--- Let's look at the 'shipped_units' and 'our_price' in those orders ---\")\n",
    "    print(orders_df[['shipped_units', 'our_price']].head())\n",
    "else:\n",
    "    print(\"🚨 The 'Orders' DataFrame is completely empty.\")\n",
    "    print(\"This confirms that no rows have a truly empty 'concession_reason'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3345da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample of the INTERMEDIATE 'weekly_sales' (sorted by most units sold) ---\n",
      "         ship_day        asin  total_units_sold  average_selling_price\n",
      "191493 2024-12-02  B08NJNKG7R              2374              42.017230\n",
      "190772 2024-12-02  B01M7S6SRA              1833               8.199542\n",
      "212703 2025-01-06  B01M7S6SRA              1817               8.040720\n",
      "152506 2024-09-30  B08NJNPR3H              1799              40.132521\n",
      "281908 2025-04-28  B01M7S6SRA              1778               7.389569\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "✅ Success! Here is the final, sorted weekly summary:\n",
      "         ship_day        asin  total_units_sold  average_selling_price  \\\n",
      "372121 2025-08-25  B0FMKFGD39               1.0             336.090000   \n",
      "369340 2025-08-25  B09XF346QZ               8.0             260.460000   \n",
      "369338 2025-08-25  B09XBY9D4D               3.0              19.896667   \n",
      "369337 2025-08-25  B09X8KWT2Z               3.0             394.110000   \n",
      "369336 2025-08-25  B09X8J7DZB               8.0             444.000000   \n",
      "\n",
      "        total_units_conceded  \n",
      "372121                   0.0  \n",
      "369340                   0.0  \n",
      "369338                   0.0  \n",
      "369337                   0.0  \n",
      "369336                   0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your real data\n",
    "full_file_path = r\"C:\\projects\\my-local-thesis\\data\\processed_data.csv\"\n",
    "df = pd.read_csv(full_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "# Convert date column\n",
    "df['ship_day'] = pd.to_datetime(df['ship_day'])\n",
    "\n",
    "# 1. Split the DataFrame\n",
    "orders_df = df[df['concession_reason'].isnull()].copy()\n",
    "returns_df = df[df['concession_reason'].notnull()].copy()\n",
    "\n",
    "# 2. Aggregate the orders data\n",
    "weekly_sales = orders_df.groupby(\n",
    "    [pd.Grouper(key='ship_day', freq='W-MON'), 'asin']\n",
    ").agg(\n",
    "    total_units_sold=('shipped_units', 'sum'),\n",
    "    average_selling_price=('our_price', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# 3. Aggregate the returns data\n",
    "weekly_returns = returns_df.groupby(\n",
    "    [pd.Grouper(key='ship_day', freq='W-MON'), 'asin']\n",
    ").agg(\n",
    "    total_units_conceded=('total_units_conceded', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# --- PROOF ---\n",
    "# Let's look at the sales summary BEFORE we merge.\n",
    "# We'll sort it to see the weeks with the most sales.\n",
    "print(\"--- Sample of the INTERMEDIATE 'weekly_sales' (sorted by most units sold) ---\")\n",
    "print(weekly_sales.sort_values(by='total_units_sold', ascending=False).head())\n",
    "print(\"-\" * 70)\n",
    "\n",
    "\n",
    "# 4. Merge the two summaries\n",
    "weekly_performance_df = pd.merge(\n",
    "    weekly_sales,\n",
    "    weekly_returns,\n",
    "    on=['ship_day', 'asin'],\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# 5. Clean up and sort the final DataFrame\n",
    "weekly_performance_df.fillna(0, inplace=True)\n",
    "weekly_performance_df.sort_values(by='ship_day', ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "print(\"\\n✅ Success! Here is the final, sorted weekly summary:\")\n",
    "print(weekly_performance_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2a89690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tavily-python\n",
      "  Downloading tavily_python-0.7.11-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: requests in c:\\projects\\my-local-thesis\\.venv\\lib\\site-packages (from tavily-python) (2.32.4)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in c:\\projects\\my-local-thesis\\.venv\\lib\\site-packages (from tavily-python) (0.9.0)\n",
      "Requirement already satisfied: httpx in c:\\projects\\my-local-thesis\\.venv\\lib\\site-packages (from tavily-python) (0.28.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\projects\\my-local-thesis\\.venv\\lib\\site-packages (from tiktoken>=0.5.1->tavily-python) (2024.11.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\projects\\my-local-thesis\\.venv\\lib\\site-packages (from requests->tavily-python) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\projects\\my-local-thesis\\.venv\\lib\\site-packages (from requests->tavily-python) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\projects\\my-local-thesis\\.venv\\lib\\site-packages (from requests->tavily-python) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\projects\\my-local-thesis\\.venv\\lib\\site-packages (from requests->tavily-python) (2025.6.15)\n",
      "Requirement already satisfied: anyio in c:\\projects\\my-local-thesis\\.venv\\lib\\site-packages (from httpx->tavily-python) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\projects\\my-local-thesis\\.venv\\lib\\site-packages (from httpx->tavily-python) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\projects\\my-local-thesis\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->tavily-python) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\projects\\my-local-thesis\\.venv\\lib\\site-packages (from anyio->httpx->tavily-python) (1.3.1)\n",
      "Downloading tavily_python-0.7.11-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: tavily-python\n",
      "Successfully installed tavily-python-0.7.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07ecbba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thefuzz\n",
      "  Downloading thefuzz-0.22.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.0.0 (from thefuzz)\n",
      "  Downloading rapidfuzz-3.14.0-cp313-cp313-win_amd64.whl.metadata (12 kB)\n",
      "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
      "  Downloading levenshtein-0.27.1-cp313-cp313-win_amd64.whl.metadata (3.6 kB)\n",
      "Downloading thefuzz-0.22.1-py3-none-any.whl (8.2 kB)\n",
      "Downloading rapidfuzz-3.14.0-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 1.6/1.7 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 8.8 MB/s eta 0:00:00\n",
      "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading levenshtein-0.27.1-cp313-cp313-win_amd64.whl (100 kB)\n",
      "Installing collected packages: rapidfuzz, thefuzz, Levenshtein, python-Levenshtein\n",
      "\n",
      "   ---------------------------------------- 0/4 [rapidfuzz]\n",
      "   ---------------------------------------- 0/4 [rapidfuzz]\n",
      "   ---------------------------------------- 0/4 [rapidfuzz]\n",
      "   ---------------------------------------- 0/4 [rapidfuzz]\n",
      "   ---------- ----------------------------- 1/4 [thefuzz]\n",
      "   ---------------------------------------- 4/4 [python-Levenshtein]\n",
      "\n",
      "Successfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.14.0 thefuzz-0.22.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install thefuzz python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eff9f77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File loaded successfully with the correct separator.\n",
      "Preparing to ingest product data...\n",
      "Cleared the 'products' table.\n",
      "✅ Successfully loaded 17989 unique products into the database.\n",
      "--------------------------------------------------\n",
      "Preparing to ingest weekly performance data...\n",
      "✅ Successfully loaded weekly performance summary into the database.\n",
      "--------------------------------------------------\n",
      "--- Testing financial_data_tool with query: 'What were the sales for ASIN B08NJNKG7R in December 2024?' ---\n",
      "\n",
      "--- Result from the tool ---\n",
      "Sorry, I couldn't find any relevant data on this product from internal sources. I will get some information in general from the LLM database.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "# --- Add the project root to Python's path ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from src.database import engine\n",
    "\n",
    "# --- Load and Prepare the Main DataFrame ---\n",
    "full_file_path = r\"C:\\projects\\my-local-thesis\\data\\processed_data.csv\"\n",
    "\n",
    "# --- THE FIX: Using the correct tab separator ---\n",
    "df = pd.read_csv(full_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "print(\"✅ File loaded successfully with the correct separator.\")\n",
    "\n",
    "# Convert date column\n",
    "df['ship_day'] = pd.to_datetime(df['ship_day'])\n",
    "\n",
    "\n",
    "# --- STEP 1: Prepare and Ingest Product Data ---\n",
    "print(\"Preparing to ingest product data...\")\n",
    "products_df = df[['asin', 'gl_name', 'product_type']].drop_duplicates(subset=['asin']).copy()\n",
    "products_df.rename(columns={'gl_name': 'product_name'}, inplace=True)\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(text(\"TRUNCATE TABLE products RESTART IDENTITY CASCADE;\"))\n",
    "    connection.commit()\n",
    "    print(\"Cleared the 'products' table.\")\n",
    "\n",
    "products_df.to_sql('products', con=engine, if_exists='append', index=False)\n",
    "print(f\"✅ Successfully loaded {len(products_df)} unique products into the database.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- STEP 2: Prepare and Ingest Weekly Performance Data ---\n",
    "print(\"Preparing to ingest weekly performance data...\")\n",
    "orders_df = df[df['concession_reason'].isnull()].copy()\n",
    "returns_df = df[df['concession_reason'].notnull()].copy()\n",
    "weekly_sales = orders_df.groupby([pd.Grouper(key='ship_day', freq='W-MON'), 'asin']).agg(total_units_sold=('shipped_units', 'sum'), average_selling_price=('our_price', 'mean')).reset_index()\n",
    "weekly_returns = returns_df.groupby([pd.Grouper(key='ship_day', freq='W-MON'), 'asin']).agg(total_units_conceded=('total_units_conceded', 'sum')).reset_index()\n",
    "weekly_performance_df = pd.merge(weekly_sales, weekly_returns, on=['ship_day', 'asin'], how='outer').fillna(0)\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(text(\"TRUNCATE TABLE weekly_performance RESTART IDENTITY;\"))\n",
    "    connection.commit()\n",
    "\n",
    "weekly_performance_df.rename(columns={'ship_day': 'week_start_date', 'total_units_conceded': 'num_reviews_received'}, inplace=True)\n",
    "columns_to_ingest = ['week_start_date', 'asin', 'total_units_sold', 'average_selling_price', 'num_reviews_received']\n",
    "weekly_performance_df[columns_to_ingest].to_sql('weekly_performance', con=engine, if_exists='append', index=False)\n",
    "print(\"✅ Successfully loaded weekly performance summary into the database.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- STEP 3: Test the Financial Data Tool ---\n",
    "from src.tools.custom_tools import financial_data_tool\n",
    "\n",
    "test_query = \"What were the sales for ASIN B08NJNKG7R in December 2024?\"\n",
    "print(f\"--- Testing financial_data_tool with query: '{test_query}' ---\")\n",
    "result = financial_data_tool(test_query)\n",
    "\n",
    "print(\"\\n--- Result from the tool ---\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e24804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PerformanceAgent: Received query: 'What were the sales for ASIN B08NJNKG7R in December 2024?' ---\n",
      "--- PerformanceAgent: Found direct ASIN match: B08NJNKG7R ---\n",
      "--- PerformanceAgent: Matched to 'Major Appliances' (ASIN: B08NJNKG7R), Date Range: 2024-12-01 to 2024-12-31 ---\n",
      "\n",
      "--- Final Result from the Tool ---\n",
      "Product Name: Major Appliances\n",
      "Performance data from 2024-12-01 to 2024-12-31:\n",
      "| week_start_date   |   total_units_sold |   average_selling_price |   num_reviews_received |\n",
      "|:------------------|-------------------:|------------------------:|-----------------------:|\n",
      "| 2024-12-02        |               2374 |                 42.0172 |                     97 |\n",
      "| 2024-12-09        |                375 |                 42.0884 |                      8 |\n",
      "| 2024-12-16        |                524 |                 42.2123 |                     21 |\n",
      "| 2024-12-23        |                 29 |                 43.3134 |                      2 |\n",
      "| 2024-12-30        |                  1 |                 49.15   |                      0 |\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "from thefuzz import process\n",
    "from functools import lru_cache\n",
    "\n",
    "# We assume 'engine' is already loaded in your notebook from a previous cell.\n",
    "# If not, you would add the path-fixing code and `from src.database import engine` here.\n",
    "\n",
    "# --- Helper Function (Copied from custom_tools.py) ---\n",
    "# We need this because the main function calls it.\n",
    "def get_dates_from_query(query: str) -> dict:\n",
    "    # For this test, we'll use a simplified version.\n",
    "    # A full version would query the DB to get the min/max dates.\n",
    "    # This is sufficient for our current test.\n",
    "    min_db_date, max_db_date = \"2021-01-01\", \"2025-12-31\" # Assuming a wide range for testing\n",
    "    \n",
    "    if \"december 2024\" in query.lower():\n",
    "        return {\"start_date\": \"2024-12-01\", \"end_date\": \"2024-12-31\"}\n",
    "    # Add more simple date logic here if needed for other tests\n",
    "    \n",
    "    # Default fallback\n",
    "    return {\"start_date\": min_db_date, \"end_date\": max_db_date}\n",
    "\n",
    "\n",
    "# --- Main Function (This is our new, upgraded version) ---\n",
    "# You can modify this function as much as you want.\n",
    "def financial_data_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns weekly performance data for a product within a specified date range.\n",
    "    It first searches for a direct ASIN match in the query. If none is found,\n",
    "    it uses fuzzy matching to find products by name.\n",
    "    \"\"\"\n",
    "    print(f\"--- PerformanceAgent: Received query: '{query}' ---\")\n",
    "    try:\n",
    "        product_id_found = None\n",
    "        product_name_found = None\n",
    "        \n",
    "        # Step 1: Look for a direct ASIN match\n",
    "        asin_match = re.search(r'\\bB[A-Z0-9]{9}\\b', query)\n",
    "        if asin_match:\n",
    "            product_id_found = asin_match.group(0)\n",
    "            print(f\"--- PerformanceAgent: Found direct ASIN match: {product_id_found} ---\")\n",
    "            with engine.connect() as conn:\n",
    "                result = conn.execute(text(\"SELECT product_name FROM products WHERE asin = :asin\"), {\"asin\": product_id_found}).fetchone()\n",
    "                if result:\n",
    "                    product_name_found = result[0]\n",
    "                else:\n",
    "                    return f\"Found ASIN {product_id_found} in query, but this product does not exist in the database.\"\n",
    "\n",
    "        # Step 2: Fallback to fuzzy name matching\n",
    "        if not product_id_found:\n",
    "            print(\"--- PerformanceAgent: No ASIN found, falling back to fuzzy name search. ---\")\n",
    "            products_df = pd.read_sql_query(\"SELECT asin, product_name FROM products;\", engine)\n",
    "            product_choices = products_df['product_name'].tolist()\n",
    "            best_match = process.extractOne(query, product_choices)\n",
    "            if best_match and best_match[1] >= 80:\n",
    "                product_name_found = best_match[0]\n",
    "                product_id_found = products_df[products_df['product_name'] == product_name_found]['asin'].iloc[0]\n",
    "            else:\n",
    "                return \"Sorry, I couldn't find a matching product by name.\"\n",
    "\n",
    "        if not product_id_found:\n",
    "             return \"Sorry, I couldn't identify a product from your query.\"\n",
    "\n",
    "        # Step 3: Get dates and query the data\n",
    "        dates = get_dates_from_query(query)\n",
    "        start_date, end_date = dates['start_date'], dates['end_date']\n",
    "        print(f\"--- PerformanceAgent: Matched to '{product_name_found}' (ASIN: {product_id_found}), Date Range: {start_date} to {end_date} ---\")\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "            sql_query = \"\"\"\n",
    "                SELECT week_start_date, total_units_sold, average_selling_price, num_reviews_received \n",
    "                FROM weekly_performance \n",
    "                WHERE asin = :asin AND week_start_date BETWEEN :start_date AND :end_date\n",
    "                ORDER BY week_start_date;\n",
    "            \"\"\"\n",
    "            params = {'asin': product_id_found, 'start_date': start_date, 'end_date': end_date}\n",
    "            data = pd.read_sql_query(text(sql_query), conn, params=params)\n",
    "        \n",
    "        if data.empty:\n",
    "            return f\"No performance data found for {product_name_found} in the period ({start_date} to {end_date}).\"\n",
    "        \n",
    "        return f\"Product Name: {product_name_found}\\nPerformance data from {start_date} to {end_date}:\\n{data.to_markdown(index=False)}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return f\"Error fetching performance data: {e}\"\n",
    "\n",
    "\n",
    "# --- Test Call ---\n",
    "# Now we call the function we just defined in this cell\n",
    "test_query = \"What were the sales for ASIN B08NJNKG7R in December 2024?\"\n",
    "result = financial_data_tool(test_query)\n",
    "\n",
    "print(\"\\n--- Final Result from the Tool ---\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c61f3469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected path after going up one level: c:\\projects\\my-local-thesis\\data\\processed_data.csv\n",
      "\n",
      "✅ Data loaded successfully!\n",
      "--------------------------------------------------\n",
      "Total rows in the original file: 3,962,696\n",
      "Number of rows identified as ORDERS: 3,709,223\n",
      "Number of rows identified as CONCESSIONS: 253,473\n",
      "--------------------------------------------------\n",
      "✅ Verification successful: All rows have been classified.\n",
      "\n",
      "--- Sample of 'Orders' Data ---\n",
      "         asin             ship_day  shipped_units  our_price concession_reason\n",
      "0  B098K6C12Z  2025-04-10 00:00:00              1      15.96               NaN\n",
      "1  B07WGS2NNC  2024-01-13 00:00:00              1      13.44               NaN\n",
      "2  B01M7S6SRA  2025-04-08 00:00:00              1       6.40               NaN\n",
      "3  B0BGQK683X  2025-08-18 00:00:00              1       7.97               NaN\n",
      "4  B09J57TRP6  2025-08-04 00:00:00              1      14.25               NaN\n",
      "\n",
      "--- Sample of 'Concessions' Data ---\n",
      "               asin             ship_day  shipped_units  total_units_conceded  \\\n",
      "1284449  B09N9X8YWJ  2025-05-12 00:00:00              0                   1.0   \n",
      "1284450  B0BVQLR433  2024-02-01 00:00:00              0                   1.0   \n",
      "1284451  B08JVCNXRD  2025-01-25 00:00:00              0                   1.0   \n",
      "1284452  B0CQN6YS7J  2024-11-12 00:00:00              0                   1.0   \n",
      "1284453  B093BK8TBP  2024-03-18 00:00:00              0                   1.0   \n",
      "\n",
      "                                       concession_reason  \n",
      "1284449                                 Accidental order  \n",
      "1284450                                  No reason given  \n",
      "1284451                                  No reason given  \n",
      "1284452  Incompatible or not useful for intended purpose  \n",
      "1284453                 Defective/Does not work properly  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Automatic Path Detection ---\n",
    "# Get the current directory (c:\\...\\my-local-thesis\\test)\n",
    "current_dir = os.getcwd() \n",
    "# Go UP one level to get the correct project root (c:\\...\\my-local-thesis)\n",
    "project_root = os.path.dirname(current_dir)\n",
    "# Construct the correct path to the data file\n",
    "file_path = os.path.join(project_root, 'data', 'processed_data.csv')\n",
    "\n",
    "# --- SCRIPT ---\n",
    "print(f\"Corrected path after going up one level: {file_path}\\n\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path, sep='\\t', low_memory=False)\n",
    "    \n",
    "    print(\"✅ Data loaded successfully!\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "    # --- Verification Step ---\n",
    "    orders_df = df[df['concession_reason'].isnull()].copy()\n",
    "    concessions_df = df[df['concession_reason'].notnull()].copy()\n",
    "\n",
    "    print(f\"Total rows in the original file: {len(df):,}\")\n",
    "    print(f\"Number of rows identified as ORDERS: {len(orders_df):,}\")\n",
    "    print(f\"Number of rows identified as CONCESSIONS: {len(concessions_df):,}\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    \n",
    "    if len(df) == len(orders_df) + len(concessions_df):\n",
    "        print(\"✅ Verification successful: All rows have been classified.\")\n",
    "    else:\n",
    "        print(\"❌ Verification failed: Row counts do not match.\")\n",
    "\n",
    "    print(\"\\n--- Sample of 'Orders' Data ---\")\n",
    "    print(orders_df[['asin', 'ship_day', 'shipped_units', 'our_price', 'concession_reason']].head())\n",
    "\n",
    "    print(\"\\n--- Sample of 'Concessions' Data ---\")\n",
    "    print(concessions_df[['asin', 'ship_day', 'shipped_units', 'total_units_conceded', 'concession_reason']].head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: The file was still not found at '{file_path}'.\")\n",
    "    print(\"Please double-check your folder structure.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
